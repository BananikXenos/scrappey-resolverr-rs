# scrappey-resolverr-rs üöÄü¶Ä

A high-performance, Rust-based, FlareSolverr-compatible API for bypassing anti-bot challenges (Cloudflare, DDoS-Guard, etc.) using a headful Chrome browser running inside a virtual display (via `transparent` and `xvfb-run`), with Scrappey fallback and built-in authenticated HTTP proxy bridging.

---

## Overview üìñ

**scrappey-resolverr-rs** is a modern, Docker-ready replacement for [FlareSolverr](https://github.com/FlareSolverr/FlareSolverr), written in Rust for speed and reliability. It exposes a FlareSolverr-compatible HTTP API, orchestrates a headful Chrome browser running inside a virtual display (using the `transparent` library and `xvfb-run`) to solve anti-bot challenges, and can fall back to the Scrappey API for advanced bypassing. It also includes a local HTTP-to-authenticated-HTTP proxy bridge, making it easy to use authenticated proxies with browser automation.

---

## How it Works ‚öôÔ∏è

1. **API Requests:**
   The server exposes endpoints compatible with FlareSolverr (`/v1`, `/health`, `/`).

2. **Challenge Handling:**
   - Receives a request to fetch a URL.
   - Launches a headful Chrome session (not headless) via `chromedriver`, running inside a virtual display using the `transparent` library and `xvfb-run`.
   - Navigates to the target URL, handling cookies and user-agent spoofing.
   - Detects and solves anti-bot challenges (Cloudflare, DDoS-Guard) automatically.
   - If browser-based solving fails, falls back to the [Scrappey](https://scrappey.com/) API.

3. **Proxy Bridge:**
   - Runs a local HTTP proxy on port `8080` that forwards requests to an upstream authenticated HTTP proxy (as configured in Docker).
   - Chrome is configured to use this bridge, enabling authenticated proxy support.

4. **Persistence:**
   - Cookies and user-agent are persisted to disk (`/data/persistent.json`) for session continuity.
   - Failure screenshots are automatically captured when challenges fail (saved to `/data/screenshots/`).

---

## Architecture üèóÔ∏è

- **Rust (tokio, axum):** High-performance async server and proxy.
- **Headful Chrome (chromedriver + transparent + xvfb-run):** Real browser automation for challenge solving, running in a virtual display (not headless).
- **Scrappey API:** Fallback for advanced anti-bot bypass.
- **HTTP Proxy Bridge:** Local proxy for authenticated upstream proxies.
- **Dockerized:** All dependencies (Chrome, chromedriver, proxy) managed via Docker Compose.

**Key Components:**
- `src/main.rs` ‚Äî Entrypoint, config, server, and process management.
- `src/flaresolverr.rs` ‚Äî FlareSolverr-compatible API handlers.
- `src/browser.rs` ‚Äî Browser automation and challenge logic.
- `src/challenge.rs` ‚Äî Challenge detection and solving.
- `src/fwd_proxy.rs` ‚Äî HTTP proxy bridge implementation.
- `src/scrappey.rs` ‚Äî Scrappey API client.

---

## Installation üê≥

### Prerequisites üì¶

- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)

### Quick Start üö¶

#### Option 1: Use Prebuilt Docker Image (Recommended)

You can use the prebuilt image from GitHub Container Registry without building locally:

1. **Configure environment variables:**
   Edit `docker-compose.yml` and set:
   - `SCRAPPEY_API_KEY` (get from [Scrappey](https://scrappey.com/))
   - `PROXY_HOST`, `PROXY_PORT`, `PROXY_USERNAME`, `PROXY_PASSWORD` (your HTTP proxy credentials)

2. **Update your `docker-compose.yml`:**
   In the `scrappey-resolverr` service section, set the image to:
   ```yaml
   image: ghcr.io/bananikxenos/scrappey-resolverr-rs:release
   ```
   Remove or comment out any `build:` lines for this service.

3. **Start the services:**
   ```sh
   docker-compose up -d
   ```

   This will:
   - Start an authenticated Squid proxy (`proxy` service)
   - Pull and run the prebuilt `scrappey-resolverr-rs` image (`scrappey-resolverr` service)
   - Launch Chrome and chromedriver inside the container

4. **API will be available at:**
   `http://localhost:8191` üéØ

---

#### Option 2: Build Locally

1. **Clone the repository:**
   ```sh
   git clone <this-repo-url>
   cd scrappey-resolverr-rs
   ```

2. **Configure environment variables:**
   Edit `docker-compose.yml` and set:
   - `SCRAPPEY_API_KEY` (get from [Scrappey](https://scrappey.com/))
   - `PROXY_HOST`, `PROXY_PORT`, `PROXY_USERNAME`, `PROXY_PASSWORD` (your HTTP proxy credentials)

3. **Start the services:**
   ```sh
   docker-compose up --build
   ```

   This will:
   - Start an authenticated Squid proxy (`proxy` service)
   - Build and run `scrappey-resolverr-rs` (`scrappey-resolverr` service)
   - Launch Chrome and chromedriver inside the container

4. **API will be available at:**
   `http://localhost:8191` üéØ

---

## Usage Examples üßë‚Äçüíª

### Health Check ‚ù§Ô∏è

```sh
curl http://localhost:8191/health
```

### Solve a Challenge (GET request) üõ°Ô∏è

```sh
curl -X POST http://localhost:8191/v1 \
  -H 'Content-Type: application/json' \
  -d '{
    "cmd": "request.get",
    "url": "https://protected-site.com/",
    "maxTimeout": 60000
  }'
```

### Example Response üì¶

```json
{
  "status": "ok",
  "message": "Challenge solved!",
  "solution": {
    "url": "https://protected-site.com/",
    "status": 200,
    "headers": {},
    "response": "<html>...</html>",
    "cookies": [
      {
        "name": "...",
        "value": "...",
        "domain": "...",
        "path": "/",
        "expires": 1712345678,
        "httpOnly": false,
        "secure": true,
        "sameSite": "Lax"
      }
    ],
    "userAgent": "Mozilla/5.0 ..."
  }
}
```

---

## Configuration üîß

### Environment Variables

- `SCRAPPEY_API_KEY` - Your Scrappey API key (required)
- `PROXY_HOST` - HTTP proxy hostname (required)
- `PROXY_PORT` - HTTP proxy port (required)
- `PROXY_USERNAME` - HTTP proxy username (optional)
- `PROXY_PASSWORD` - HTTP proxy password (optional)
- `DATA_PATH` - Path to persistent data file (default: `/data/persistent.json`)
- `CAPTURE_FAILURE_SCREENSHOTS` - Enable/disable failure screenshots (default: `true`)
- `SCREENSHOT_DIR` - Directory for failure screenshots (default: `/data/screenshots`)
- `HOST` - Server bind address (default: `0.0.0.0`)
- `PORT` - Server port (default: `8191`)

### Failure Screenshots üì∏

When challenge resolution fails, the system automatically captures screenshots for debugging purposes. These are saved with timestamps and domain names:

- **Location:** `/data/screenshots/` (configurable via `SCREENSHOT_DIR`)
- **Format:** `failure_{domain}_{timestamp}.png` or `ddos_guard_failure_{domain}_{timestamp}.png`
- **Control:** Set `CAPTURE_FAILURE_SCREENSHOTS=false` to disable

Example screenshot filename: `failure_example.com_20240315_143022.png`

## Notes

- **Persistence:** Cookies and user-agent are saved in `/data/persistent.json` (mounted as a Docker volume).
- **Proxy:** Chrome always connects to the local proxy bridge (`127.0.0.1:8080`), which forwards to your configured authenticated proxy.
- **Fallback:** If browser-based solving fails, Scrappey API is used (requires a valid API key and balance).
- **Screenshots:** Failure screenshots are automatically captured for debugging when challenges cannot be solved.
- **Sessions:** Session management is not implemented (stateless per request).

---

## Prowlarr Configuration ü¶Åüîß

To use scrappey-resolverr-rs with [Prowlarr](https://github.com/Prowlarr/Prowlarr):

### 1. Go to Settings ‚Üí Indexers ‚öôÔ∏è

### 2. Add Two Proxies üß©

**FlareSolverr Proxy:** üå©Ô∏è
- **Host:** Locally connectable IP of your scrappey-resolverr instance (e.g., the LAN IP or Docker network IP accessible from your Prowlarr host)
- **Port:** 8191 (default FlareSolverr port)
- **Tags:** a tag like `scrappey`

**HTTP Proxy:** üåê
- **Host:** Your **publicly exposed** proxy address (the proxy must be accessible from the public internet, as Scrappey will use it externally to act on your IP)
- **Port:** Your `PROXY_PORT`
- **Username:** Your `PROXY_USERNAME`
- **Password:** Your `PROXY_PASSWORD`
- **Tags:** a tag like `proxy`

### 3. For Each Indexer That Needs Cloudflare Bypass üõ°Ô∏è

- Edit the indexer settings
- Add both tags you created to the "Tags" field

This ensures that requests for those indexers are routed through both the FlareSolverr-compatible API and your authenticated HTTP proxy.

**Why is the HTTP proxy required?** ü§î

The HTTP proxy is essential for maintaining **IP persistence**. This means that cookies and sessions remain valid across requests, as all browser and API traffic is routed through the same outgoing IP. üç™üîí As a result, cookies and user-agents do **not** need to be refreshed on every call, which dramatically reduces the number of Scrappey API calls required. This leads to more stable scraping sessions and significant savings on Scrappey usage. üí∏‚ú®

---

## License üìÑ

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
